
/* ----------------------------------------------------------------------- *//**

@file prediction_metrics.sql_in

@brief Implementation of various prediction accuracy metrics.

@author Written by Michael Brand
@date 21 Oct 2014

 *//* ----------------------------------------------------------------------- */

/**
@addtogroup grp_mf_mae

@brief Mean Absolute Error.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_mae_syntax">Syntax</a>
<li class="level1"><a href="#mf_mae_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_mae_example">Example</a>
</ul>
</div>

@about
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute difference between them, this being
the Mean Absolute Error (MAE) prediction accuracy metric.

@anchor mf_mae_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_mae(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.

@returns The MAE value.

@anchor mf_mae_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute difference between them, this being
the Mean Absolute Error (MAE) prediction accuracy metric.

Rows containing NULLs are ignored.

@anchor mf_mae_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric.
\code
user=# SELECT mf_mae(pred,obs) FROM test_set;
 mf_mae
--------
 13.825
(1 row)
\endcode

 */


-- begin of mf_mae definition

DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM AS (
    arg_sum FLOAT8,
    arg_num INTEGER
);

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mae_transition(
    oldval PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM,
    newprediction FLOAT8,
    newobserved FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_sum+abs($2-$3),$1.arg_num+1)::PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mae_combine(
    old1 PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM,
    old2 PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM)
RETURNS PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM AS
$$
    SELECT ($1.arg_sum+$2.arg_sum,$1.arg_num+$2.arg_num)::PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mae_final(
    finalstate PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM)
RETURNS FLOAT8 AS
$$
    SELECT $1.arg_sum/$1.arg_num
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Mean Absolute Error.
 *
 * @about
 * An aggregate taking two columns, a "prediction" column and an "observed"
 * column and returning the mean absolute difference between them, this being
 * the Mean Absolute Error (MAE) prediction accuracy metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_mae(prediction FLOAT8, observed FLOAT8);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 *
 * @returns The MAE value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_mae(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_mae_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_mae_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_mae_final,
    INITCOND='(0.0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mae()
RETURNS TEXT AS $$
SELECT $ABC$
mf_mae: Mean Absolute Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute difference between them, this being
the Mean Absolute Error (MAE) prediction accuracy metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_mae('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mae(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_mae() ELSE $ABC$
mf_mae: Mean Absolute Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute difference between them, this being
the Mean Absolute Error (MAE) prediction accuracy metric.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_mae(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8


prediction - The column of predicted values.
observed   - The column of observed values.

Returns the MAE value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute difference between them, this being
the Mean Absolute Error (MAE) prediction accuracy metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric.

user=# SELECT PDLTOOLS_SCHEMA.mf_mae(pred,obs) FROM test_set;
 mf_mae
--------
 13.825
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;

/**
@addtogroup grp_mf_mape

@brief Mean Absolute Percentage Error.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_mape_syntax">Syntax</a>
<li class="level1"><a href="#mf_mape_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_mape_example">Example</a>
</ul>
</div>

@about
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute percentage difference between them,
this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
metric.

@anchor mf_mape_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_mape(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.

@returns The MAPE value.

@anchor mf_mape_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute percentage difference between them,
this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
metric.

Rows containing NULLs are ignored.

@anchor mf_mape_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric.
\code
user=# SELECT mf_mape(pred,obs) FROM test_set;
      mf_mape      
-------------------
 0.294578793636013
(1 row)
\endcode

 */


-- begin of mf_mape definition

DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM AS (
    arg_sum FLOAT8,
    arg_num INTEGER
);

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mape_transition(
    oldval PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM,
    newprediction FLOAT8,
    newobserved FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_sum+abs(($2-$3)/$3),$1.arg_num+1)::PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mape_combine(
    old1 PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM,
    old2 PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM)
RETURNS PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT ($1.arg_sum+$2.arg_sum,$1.arg_num+$2.arg_num)::PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mape_final(
    finalstate PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM)
RETURNS FLOAT8 AS
$$
    SELECT $1.arg_sum/$1.arg_num
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Mean Absolute Percentage Error.
 *
 * @about
 * An aggregate taking two columns, a "prediction" column and an "observed"
 * column and returning the mean absolute percentage difference between them,
 * this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
 * metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_mape(prediction FLOAT8, observed FLOAT8);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 *
 * @returns The MAPE value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_mape(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_mape_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_mape_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_mape_final,
    INITCOND='(0.0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mape()
RETURNS TEXT AS $$
SELECT $ABC$
mf_mape: Mean Absolute Percentage Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute percentage difference between them,
this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_mape('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mape(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_mape() ELSE $ABC$
mf_mape: Mean Absolute Percentage Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute percentage difference between them,
this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
metric.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_mape(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8


prediction - The column of predicted values.
observed   - The column of observed values.

Returns the MAPE value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute percentage difference between them,
this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric.

user=# SELECT PDLTOOLS_SCHEMA.mf_mape(pred,obs) FROM test_set;
      mf_mape      
-------------------
 0.294578793636013
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;

/**
@addtogroup grp_mf_mpe

@brief Mean Percentage Error.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_mpe_syntax">Syntax</a>
<li class="level1"><a href="#mf_mpe_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_mpe_example">Example</a>
</ul>
</div>

@about
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean percentage difference between them,
this being the Mean Percentage Error (MPE) prediction accuracy
metric.

@anchor mf_mpe_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_mpe(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.

@returns The MPE value.

@anchor mf_mpe_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean percentage difference between them,
this being the Mean Percentage Error (MPE) prediction accuracy
metric.

@anchor mf_mpe_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric.
\code
user=# SELECT mf_mpe(pred,obs) FROM test_set;
      mf_mpe       
-------------------
 -0.17248930032771
(1 row)
\endcode

 */


-- begin of mf_mpe definition

DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM AS (
    arg_sum FLOAT8,
    arg_num INTEGER
);

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mpe_transition(
    oldval PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM,
    newprediction FLOAT8,
    newobserved FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_sum+(($2-$3)/$3),$1.arg_num+1)::PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mpe_combine(
    old1 PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM,
    old2 PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM)
RETURNS PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT ($1.arg_sum+$2.arg_sum,$1.arg_num+$2.arg_num)::PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mpe_final(
    finalstate PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM)
RETURNS FLOAT8 AS
$$
    SELECT $1.arg_sum/$1.arg_num
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Mean Percentage Error.
 *
 * @about
 * An aggregate taking two columns, a "prediction" column and an "observed"
 * column and returning the mean percentage difference between them,
 * this being the Mean Percentage Error (MPE) prediction accuracy
 * metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_mpe(prediction FLOAT8, observed FLOAT8);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 *
 * @returns The MPE value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_mpe(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_mpe_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_mpe_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_mpe_final,
    INITCOND='(0.0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mpe()
RETURNS TEXT AS $$
SELECT $ABC$
mf_mpe: Mean Percentage Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean percentage difference between them,
this being the Mean Percentage Error (MPE) prediction accuracy
metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_mpe('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mpe(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_mpe() ELSE $ABC$
mf_mpe: Mean Percentage Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean percentage difference between them,
this being the Mean Percentage Error (MPE) prediction accuracy
metric.

Rows containing NULLs are ignored.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_mpe(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8


prediction - The column of predicted values.
observed   - The column of observed values.

Returns the MPE value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean percentage difference between them,
this being the Mean Percentage Error (MPE) prediction accuracy
metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric.

user=# SELECT PDLTOOLS_SCHEMA.mf_mpe(pred,obs) FROM test_set;
      mf_mpe       
-------------------
 -0.17248930032771
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;


/**
@addtogroup grp_mf_rmse

@brief Root Mean Square Error.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_rmse_syntax">Syntax</a>
<li class="level1"><a href="#mf_rmse_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_rmse_example">Example</a>
</ul>
</div>

@about
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the root mean square deviation between them,
this being the Root Mean Square Error (RMSE) prediction accuracy
metric.

@anchor mf_rmse_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_rmse(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.

@returns The RMSE value.

@anchor mf_rmse_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the root mean square deviation between them,
this being the Root Mean Square Error (RMSE) prediction accuracy
metric.

Rows containing NULLs are ignored.

@anchor mf_rmse_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric.
\code
user=# SELECT mf_rmse(pred,obs) FROM test_set;
     mf_rmse      
------------------
 14.8442749907161
(1 row)
\endcode

 */


-- begin of mf_rmse definition

DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM AS (
    arg_sum FLOAT8,
    arg_num INTEGER
);

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_rmse_transition(
    oldval PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM,
    newprediction FLOAT8,
    newobserved FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_sum+(($2-$3)^2),$1.arg_num+1)::PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_rmse_combine(
    old1 PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM,
    old2 PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM)
RETURNS PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT ($1.arg_sum+$2.arg_sum,$1.arg_num+$2.arg_num)::PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_rmse_final(
    finalstate PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM)
RETURNS FLOAT8 AS
$$
    SELECT sqrt($1.arg_sum/$1.arg_num)
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Root Mean Square Error.
 *
 * @about
 * An aggregate taking two columns, a "prediction" column and an "observed"
 * column and returning the root mean square deviation between them,
 * this being the Root Mean Square Error (RMSE) prediction accuracy
 * metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_rmse(prediction FLOAT8, observed FLOAT8);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 *
 * @returns The RMSE value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_rmse(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_rmse_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_rmse_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_rmse_final,
    INITCOND='(0.0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_rmse()
RETURNS TEXT AS $$
SELECT $ABC$
mf_rmse: Root Mean Square Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the root mean square deviation between them,
this being the Root Mean Square Error (RMSE) prediction accuracy
metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_rmse('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_rmse(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_rmse() ELSE $ABC$
mf_rmse: Root Mean Square Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the root mean square deviation between them,
this being the Root Mean Square Error (RMSE) prediction accuracy
metric.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_rmse(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8


prediction - The column of predicted values.
observed   - The column of observed values.

Returns the RMSE value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the root mean square deviation between them,
this being the Root Mean Square Error (RMSE) prediction accuracy
metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric.

user=# SELECT PDLTOOLS_SCHEMA.mf_rmse(pred,obs) FROM test_set;
     mf_rmse      
------------------
 14.8442749907161
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;


/**
@addtogroup grp_mf_r2

@brief R-squared.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_r2_syntax">Syntax</a>
<li class="level1"><a href="#mf_r2_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_r2_example">Example</a>
</ul>
</div>

@about
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the coefficient of determination between them,
this being the R-squared (R^2) prediction accuracy metric.

@anchor mf_r2_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_r2(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.

@returns The R2 value.

@anchor mf_r2_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the coefficient of determination between them,
this being the R-squared (R^2) prediction accuracy metric.

Rows containing NULLs are ignored.

@anchor mf_r2_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric.
\code
user=# SELECT mf_r2(pred,obs) FROM test_set;
       mf_r2       
-------------------
 0.279929088443373
(1 row)
\endcode

 */


-- begin of mf_r2 definition

DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8 CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8 AS (
    arg_yy FLOAT8, /* f=predicted, y=observed */
    arg_y FLOAT8,
    arg_fy FLOAT8,
    arg_ff FLOAT8,
    arg_num INTEGER
);

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_r2_transition(
    oldval PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8,
    newprediction FLOAT8,
    newobserved FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8 AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_yy+$3^2,$1.arg_y+$3,
                      $1.arg_fy+$2*$3,$1.arg_ff+$2^2,
                      $1.arg_num+1)::PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_r2_combine(
    old1 PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8,
    old2 PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8 AS
$$
    SELECT ($1.arg_yy+$2.arg_yy,$1.arg_y+$2.arg_y,
            $1.arg_fy+$2.arg_fy,$1.arg_ff+$2.arg_ff,
            $1.arg_num+$2.arg_num)::PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_r2_final(
    finalstate PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8)
RETURNS FLOAT8 AS
$$
    SELECT 1.0-($1.arg_yy-2*$1.arg_fy+$1.arg_ff)/
               ($1.arg_yy-$1.arg_y^2/$1.arg_num)
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Coefficient of determination (R^2)
 *
 * @about
 * An aggregate taking two columns, a "prediction" column and an "observed"
 * column and returning the coefficient of determination between them,
 * this being the R-squared (R^2) prediction accuracy metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_r2(prediction FLOAT8, observed FLOAT8);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 *
 * @returns The R^2 value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_r2(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_r2_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_r2_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_r2_final,
    INITCOND='(0.0,0.0,0.0,0.0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_r2()
RETURNS TEXT AS $$
SELECT $ABC$
mf_r2: Coefficient of determination (R^2).

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the coefficient of determination between them,
this being the R-squared (R^2) prediction accuracy metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_r2('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_r2(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_r2() ELSE $ABC$
mf_r2: Coefficient of determination (R^2).

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the coefficient of determination between them,
this being the R-squared (R^2) prediction accuracy metric.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_r2(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8


prediction - The column of predicted values.
observed   - The column of observed values.

Returns the R^2 value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the coefficient of determination between them,
this being the R-squared (R^2) prediction accuracy metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric.

user=# SELECT PDLTOOLS_SCHEMA.mf_r2(pred,obs) FROM test_set;
       mf_r2       
-------------------
 0.279929088443373
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;

/**
@addtogroup grp_mf_adjusted_r2

@brief Adjusted R-squared.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_adjusted_r2_syntax">Syntax</a>
<li class="level1"><a href="#mf_adjusted_r2_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_adjusted_r2_example">Example</a>
</ul>
</div>

@about
An aggregate calculating the adjusted R-squared prediction accuracy
metric.

@anchor mf_adjusted_r2_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_adjusted_r2(prediction FLOAT8, observed FLOAT8,
                         num_predictors INTEGER, training_size INTEGER)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.
@param num_predictors The number of parameters in the predicting model, not counting the constant term. Should be an integer constant.
@param training_size The number of rows used for training, excluding any NULL rows. Should be an integer constant.

@returns The adjusted R2 value.

@anchor mf_adjusted_r2_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column, as well as two integers describing the degrees of freedom of the model
and the size of the training set over which it was developed, and returning
the adjusted R-squared prediction accuracy metric.

Rows containing NULLs are ignored.

@anchor mf_adjusted_r2_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric. The parameter '3' indicates
that other than the constant term, the model has three parameters (e.g., it
may take the form 7+5x+39y+0.91z). The parameter '100' indicates that the
model was trained on a traininst set with 100 rows (excluding any NULL rows).
Neither of these parameters can be deduced from the predicted values and
the test set data alone.
\code
user=# SELECT mf_adjusted_r2(pred,obs,3,100) FROM test_set;
  mf_adjusted_r2   
-------------------
 0.257426872457228
(1 row)
\endcode

 */


-- begin of mf_adjusted_r2 definition

DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8 CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8 AS (
    arg_yy FLOAT8, /* f=predicted, y=observed */
    arg_y FLOAT8,
    arg_fy FLOAT8,
    arg_ff FLOAT8,
    arg_num INTEGER,
    arg_p INTEGER, /* p=parameters, n=training size */
    arg_n INTEGER
);

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_transition(
    oldval PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8,
    newprediction FLOAT8,
    newobserved FLOAT8,
    newp INTEGER,
    newn INTEGER)
RETURNS PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8 AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_yy+$3^2,$1.arg_y+$3,
                      $1.arg_fy+$2*$3,$1.arg_ff+$2^2,
                      $1.arg_num+1,
                      greatest($1.arg_p,$4),
                      greatest($1.arg_n,$5))::PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_combine(
    old1 PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8,
    old2 PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8 AS
$$
    SELECT ($1.arg_yy+$2.arg_yy,$1.arg_y+$2.arg_y,
            $1.arg_fy+$2.arg_fy,$1.arg_ff+$2.arg_ff,
            $1.arg_num+$2.arg_num,
            greatest($1.arg_p,$2.arg_p),
            greatest($1.arg_n,$2.arg_n))::PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_final(
    finalstate PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8)
RETURNS FLOAT8 AS
$$
    SELECT 1.0-($1.arg_yy-2*$1.arg_fy+$1.arg_ff)*($1.arg_n-1)/
               (($1.arg_yy-$1.arg_y^2/$1.arg_num)*($1.arg_n-$1.arg_p-1))
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Adjusted R^2
 *
 * @about
 * An aggregate calculating the Adjusted R-squared prediction accuracy metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_adjusted_r2(prediction FLOAT8, observed FLOAT8,
 *                          num_predictors INTEGER, training_size INTEGER);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 * @param num_predictors The number of parameters in the predicting model, not counting the constant term. Should be an integer constant.
 * @param training_size The number of rows used for training, excluding any NULL rows. Should be an integer constant.
 *
 * @returns The adjusted R^2 value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_adjusted_r2(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8,
                                        /*+ num_predictors */ INTEGER,
                                        /*+ training_size */ INTEGER) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_final,
    INITCOND='(0.0,0.0,0.0,0.0,0,0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_adjusted_r2()
RETURNS TEXT AS $$
SELECT $ABC$
mf_adjusted_r2: Adjusted R^2.

An aggregate calculating the adjusted R-squared prediction accuracy metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_adjusted_r2('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_adjusted_r2(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_adjusted_r2() ELSE $ABC$
mf_adjusted_r2: Adjusted R^2.

An aggregate taking two columns, a "prediction" column and an "observed"
column, as well as two integers describing the degrees of freedom of the model
and the size of the training set over which it was developed, and returning
the adjusted R-squared prediction accuracy metric.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_adjusted_r2(prediction FLOAT8, observed FLOAT8,
                                  num_predictors INTEGER, trainig_size INTEGER)
RETURNS FLOAT8


prediction     - The column of predicted values.
observed       - The column of observed values.
num_predictors - The number of parameters in the predicting model, not
                 counting the constant term. Should be an integer constant.
training_size  - The number of rows used for training, excluding any NULL
                 rows. Should be an integer constant.

Returns the adjusted R^2 value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column, as well as two integers describing the degrees of freedom of the model
and the size of the training set over which it was developed, and returning
the adjusted R-squared prediction accuracy metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric. The parameter '3' indicates
that other than the constant term, the model has three parameters (e.g., it
may take the form 7+5x+39y+0.91z). The parameter '100' indicates that the
model was trained on a traininst set with 100 rows (excluding any NULL rows).
Neither of these parameters can be deduced from the predicted values and
the test set data alone.

user=# SELECT PDLTOOLS_SCHEMA.mf_adjusted_r2(pred,obs,3,100) FROM test_set;
  mf_adjusted_r2   
-------------------
 0.257426872457228
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;

/**
@addtogroup grp_mf_binary_classifier

@brief Metrics for binary classification.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_binary_classifier_syntax">Syntax</a>
<li class="level1"><a href="#mf_binary_classifier_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_binary_classifier_example">Example</a>
</ul>
</div>

@about
A function calculating a collection of prediction metrics relevant for
binary classification.

@anchor mf_binary_classifier_syntax
@par Syntax
<pre class="syntax">
FUNCTION mf_binary_classifier(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              truth_value INTEGER,
                              out_table TEXT
                              [, grouping_col TEXT])
RETURNS VOID
</pre>

@param in_table Name of the input table.
@param prediction_col Name of the column of predicted values.
@param observed_col Name of the column of observed values.
@param truth_value Value in table that indicates a positive result, as a string.
@param out_table Name of the output table.
@param grouping_col Optional argument, indicating which column(s) to group by.
                    If grouping by multiple columns, these should be passed
                    as a comma-separated list.

@anchor mf_binary_classifier_usage
@usage
A function calculating the true positive count (TP), true negative count (TN),
false positive count (FP), false negative count (FN),
true positive rate [=sensitivity = hit rate = recall] (TPR),
true negative rate [=specificity] (TNR),
positive predictive value [=precision] (PPV),
negative predictive value (NPV), false positive rate [=fall-out] (FPR),
false discovery rate (FDR), false negative rate [=miss rate] (FNR),
accuracy (ACC), F1 score (F1) metrics and area under the ROC curve (AUC).

These are given as columns in the output table. If grouping columns are
specified, they appear in the output as additional columns, with their
original names.

The definitions of the various metrics are as follows.

- \f$\textit{tp}\f$ is the count of correctly-classified positives.
- \f$\textit{tn}\f$ is the count of correctly-classified negatives.
- \f$\textit{fp}\f$ is the count of misclassified negatives.
- \f$\textit{fn}\f$ is the count of misclassified positives.
- \f$\textit{tpr}=\textit{tp}/(\textit{tp}+\textit{fn})\f$.
- \f$\textit{tnr}=\textit{tn}/(\textit{fp}+\textit{tn})\f$.
- \f$\textit{ppv}=\textit{tp}/(\textit{tp}+\textit{fp})\f$.
- \f$\textit{npv}=\textit{tn}/(\textit{tn}+\textit{fn})\f$.
- \f$\textit{fpr}=\textit{fp}/(\textit{fp}+\textit{tn})\f$.
- \f$\textit{fdr}=1-\textit{ppv}\f$.
- \f$\textit{fnr}=\textit{fn}/(\textit{fn}+\textit{tp})\f$.
- \f$\textit{acc}=(\textit{tp}+\textit{tn})/(\textit{tp}+\textit{tn}+\textit{fp}+\textit{fn})\f$.
- \f$\textit{f1}=2*\textit{tp}/(2*\textit{tp}+\textit{fp}+\textit{fn})\f$.
- \f$\textit{auc}=(\textit{fpr}+\textit{fnr})/2\f$.

Rows containing NULLs are ignored. The predicted and observed columns can be
of any type. The "truth value" should be a constant of the same type, passed
into the function as a string.

@anchor mf_binary_classifier_example
@examp

Assume we have trained a model and used it to reach binary predictions on
certain values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we will create the following dummy test_set table:

\code
user=# CREATE TABLE test_set AS SELECT (x+y)%(m1+2*m2) AS pred,
user-#                                  (x*y)%(m1+2*m2) AS obs,
user-#                                   m1,m2
user-# FROM generate_series(0,1) m1, generate_series(1,2) m2,
user-#      generate_series(0,59) x, generate_series(0,59) y
user-# DISTRIBUTED RANDOMLY;
SELECT 14400
\endcode

In this toy example, "obs" takes the place of the observed parameter, and
"pred" takes the place of its approximation by a model. We are interested
in determining the quality of this approximation for predicting the value '0'.

\code
user=# SELECT mf_binary_classifier('test_set','pred','obs','0','out_table');
 mf_binary_classifier   
----------------------
(1 row)

user=# \x
Expanded display is on.
user=# SELECT * FROM out_table;
-[ RECORD 1 ]---------------
tp  | 1894
tn  | 3878
fp  | 2726
fn  | 5902
tpr | 0.24294510005130836326
tnr | 0.58721986674742580254
ppv | 0.40995670995670995671
npv | 0.39652351738241308793
fpr | 0.41278013325257419746
fdr | 0.59004329004329004329
fnr | 0.75705489994869163674
acc | 0.40083333333333333333
f1  | 0.30509020618556701031
auc | 0.41508248339936708290

user=# \x
Expanded display is off.
\endcode

We now repeat the same analysis, but using "m1" and "m2" as grouping columns.

\code
user=# SELECT mf_binary_classifier('test_set','pred','obs','0','out_table2',
user-#                             'm1,m2');
 mf_binary_classifier   
----------------------
(1 row)

user=# \x
Expanded display is on.
user=# SELECT * FROM out_table2;
-[ RECORD 1 ]---------------
tp  | 400
tn  | 800
fp  | 800
fn  | 1600
m1  | 1
m2  | 1
tpr | 0.20000000000000000000
tnr | 0.50000000000000000000
ppv | 0.33333333333333333333
npv | 0.33333333333333333333
fpr | 0.50000000000000000000
fdr | 0.66666666666666666667
fnr | 0.80000000000000000000
acc | 0.33333333333333333333
f1  | 0.25000000000000000000
auc | 0.35000000000000000000
-[ RECORD 2 ]---------------
tp  | 900
tn  | 0
fp  | 900
fn  | 1800
m1  | 0
m2  | 1
tpr | 0.33333333333333333333
tnr | 0.00000000000000000000
ppv | 0.50000000000000000000
npv | 0.00000000000000000000
fpr | 1.00000000000000000000
fdr | 0.50000000000000000000
fnr | 0.66666666666666666667
acc | 0.25000000000000000000
f1  | 0.40000000000000000000
auc | 0.16666666666666666667
-[ RECORD 3 ]---------------
tp  | 144
tn  | 1728
fp  | 576
fn  | 1152
m1  | 1
m2  | 2
tpr | 0.11111111111111111111
tnr | 0.75000000000000000000
ppv | 0.20000000000000000000
npv | 0.60000000000000000000
fpr | 0.25000000000000000000
fnr | 0.88888888888888888889
acc | 0.52000000000000000000
f1  | 0.14285714285714285714
auc | 0.43055555555555555556
-[ RECORD 4 ]---------------
tp  | 450
tn  | 1350
fp  | 450
fn  | 1350
m1  | 0
m2  | 2
tpr | 0.25000000000000000000
tnr | 0.75000000000000000000
ppv | 0.50000000000000000000
npv | 0.50000000000000000000
fpr | 0.25000000000000000000
fdr | 0.50000000000000000000
fnr | 0.75000000000000000000
acc | 0.50000000000000000000
f1  | 0.33333333333333333333
auc | 0.50000000000000000000

user=# \x
Expanded display is off.
\endcode

 */


-- begin of __pm_mf_binary_classifier_agg definition

DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER AS (
    arg_tp BIGINT,
    arg_tn BIGINT,
    arg_fp BIGINT,
    arg_fn BIGINT
);

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_binary_classifier_transition(
    oldval PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER,
    newprediction ANYELEMENT,
    newobserved ANYELEMENT,
    truth_value ANYELEMENT)
RETURNS PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                WHEN $2=$4 AND $3=$4 THEN
                  ($1.arg_tp+1,$1.arg_tn,$1.arg_fp,$1.arg_fn)::PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER
                WHEN $2<>$4 AND $3<>$4 THEN
                  ($1.arg_tp,$1.arg_tn+1,$1.arg_fp,$1.arg_fn)::PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER
                WHEN $2=$4 AND $3<>$4 THEN
                  ($1.arg_tp,$1.arg_tn,$1.arg_fp+1,$1.arg_fn)::PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER
                ELSE
                  ($1.arg_tp,$1.arg_tn,$1.arg_fp,$1.arg_fn+1)::PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_binary_classifier_combine(
    old1 PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER,
    old2 PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER)
RETURNS PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER AS
$$
    SELECT ($1.arg_tp+$2.arg_tp,$1.arg_tn+$2.arg_tn,
            $1.arg_fp+$2.arg_fp,$1.arg_fn+$2.arg_fn)
            ::PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER
$$
LANGUAGE sql IMMUTABLE;

CREATE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_binary_classifier_final(
    finalstate PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER)
RETURNS BIGINT[] AS
$$
    SELECT array[$1.arg_tp,$1.arg_tn,$1.arg_fp,$1.arg_fn]
$$
LANGUAGE sql IMMUTABLE;

/**
 * @internal
 * @brief __pm_mf_binary_classifier_agg
 *
 * @about
 * An aggregate used for calculating binary classification metrics.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_binary_classifier(prediction INTEGER, observed INTEGER,
 *                                truth_value INTEGER);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 * @param truth_value Value to be used as 'positive'.
 *
 * @returns a bigint array: [TP,TN,FP,FN].
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.__pm_mf_binary_classifier_agg(
                                        /*+ prediction */ ANYELEMENT,
                                        /*+ observed */ ANYELEMENT,
                                        /*+ truth_value */ ANYELEMENT) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_binary_classifier_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_BINARY_CLASSIFIER_INTEGER,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_binary_classifier_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_binary_classifier_final,
    INITCOND='(0,0,0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_binary_classifier(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              truth_value TEXT,
                              out_table TEXT)
RETURNS VOID
VOLATILE
STRICT
LANGUAGE PLPythonU
AS
$$
  params=dict(in_table=in_table,prediction_col=prediction_col,
              observed_col=observed_col, truth_value=truth_value,
              out_table=out_table)
  plpy.execute("""
    CREATE TABLE {out_table} AS SELECT *,
      tp*1.0/(tp+fn) AS tpr, tn*1.0/(fp+tn) AS tnr, tp*1.0/(tp+fp) AS ppv,
      tn*1.0/(tn+fn) AS npv, fp*1.0/(fp+tn) AS fpr, fp*1.0/(fp+tp) AS fdr,
      fn*1.0/(fn+tp) AS fnr,
      (tp+tn)*1.0/(tp+tn+fp+fn) AS acc, 2.0*tp/(2*tp+fp+fn) AS f1,
      ((0.5*tp/(tp+fn))+(0.5*tn/(fp+tn))) AS auc
    FROM ( 
      SELECT truth_table[1] AS tp, truth_table[2] AS tn, truth_table[3] AS fp,
        truth_table[4] AS fn FROM (
        SELECT PDLTOOLS_SCHEMA.__pm_mf_binary_classifier_agg(
          {prediction_col},{observed_col},{truth_value}) AS truth_table
        FROM {in_table}
      ) x
    ) y
    DISTRIBUTED RANDOMLY;
  """.format(**params))
$$;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_binary_classifier(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              truth_value TEXT,
                              out_table TEXT,
                              grouping_col TEXT)
RETURNS VOID
VOLATILE
STRICT
LANGUAGE PLPythonU
AS
$$
  params=dict(in_table=in_table,prediction_col=prediction_col,
              observed_col=observed_col, truth_value=truth_value,
              out_table=out_table, grouping_col=grouping_col)
  plpy.execute("""
    CREATE TABLE {out_table} AS SELECT *,
      tp*1.0/(tp+fn) AS tpr, tn*1.0/(fp+tn) AS tnr, tp*1.0/(tp+fp) AS ppv,
      tn*1.0/(tn+fn) AS npv, fp*1.0/(fp+tn) AS fpr, fp*1.0/(fp+tp) AS fdr,
      fn*1.0/(fn+tp) AS fnr,
      (tp+tn)*1.0/(tp+tn+fp+fn) AS acc, 2.0*tp/(2*tp+fp+fn) AS f1,
      ((0.5*tp/(tp+fn))+(0.5*tn/(fp+tn))) AS auc
    FROM ( 
      SELECT truth_table[1] AS tp, truth_table[2] AS tn, truth_table[3] AS fp,
        truth_table[4] AS fn, {grouping_col} FROM (
        SELECT PDLTOOLS_SCHEMA.__pm_mf_binary_classifier_agg(
          {prediction_col},{observed_col},{truth_value}) AS truth_table,
          {grouping_col}
        FROM {in_table}
        GROUP BY {grouping_col}
      ) x
    ) y
    DISTRIBUTED RANDOMLY;
  """.format(**params))
$$;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_binary_classifier()
RETURNS TEXT AS $$
SELECT $ABC$
mf_binary_classifier: Metrics for binary classification.

A function calculating a collection of prediction metrics relevant for
binary classification.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_binary_classifier('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_binary_classifier(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_binary_classifier() ELSE $ABC$
mf_binary_classifier: Metrics for binary classification.

A function calculating a collection of prediction metrics relevant for
binary classification.

Syntax
======
FUNCTION PDLTOOLS_SCHEMA.mf_binary_classifier(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              truth_value TEXT,
                              out_table TEXT
                              [, grouping_col TEXT])
RETURNS VOID

in_table       - Name of the input table.
prediction_col - Name of the column of predicted values.
observed_col   - Name of the column of observed values.
truth_value    - Value in table that indicates a positive result, as a string.
out_table      - Name of the output table.
grouping_col   - Optional argument, indicating which column(s) to group by.
                    If grouping by multiple columns, these should be passed
                    as a comma-separated list.

Usage
=====

A function calculating the true positive count (TP), true negative count (TN),
false positive count (FP), false negative count (FN),
true positive rate [=sensitivity = hit rate = recall] (TPR),
true negative rate [=specificity] (TNR),
positive predictive value [=precision] (PPV),
negative predictive value (NPV), false positive rate [=fall-out] (FPR),
false discovery rate (FDR), false negative rate [=miss rate] (FNR),
accuracy (ACC), F1 score (F1) metrics and area under the ROC curve (AUC).

These are given as columns in the output table. If grouping columns are
specified, they appear in the output as additional columns, with their
original names.

The definitions of the various metrics are as follows.

- tp is the count of correctly-classified positives.
- tn is the count of correctly-classified negatives.
- fp is the count of misclassified negatives.
- fn is the count of misclassified positives.
- tpr=tp/(tp+fn).
- tnr=tn/(fp+tn).
- ppv=tp/(tp+fp).
- npv=tn/(tn+fn).
- fpr=fp/(fp+tn).
- fdr=1-ppv.
- fnr=fn/(fn+tp).
- acc=(tp+tn)/(tp+tn+fp+fn).
- f1=2*tp/(2*tp+fp+fn).
- auc=(fpr+fnr)/2.

Rows containing NULLs are ignored. The predicted and observed columns can be
of any type. The "truth value" should be a constant of the same type, passed
into the function as a string.

Example
=======
Assume we have trained a model and used it to reach binary predictions on
certain values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we will create the following dummy test_set table:

user=# CREATE TABLE test_set AS SELECT (x+y)%(m1+2*m2) AS pred,
user-#                                  (x*y)%(m1+2*m2) AS obs,
user-#                                   m1,m2
user-# FROM generate_series(0,1) m1, generate_series(1,2) m2,
user-#      generate_series(0,59) x, generate_series(0,59) y
user-# DISTRIBUTED RANDOMLY;
SELECT 14400

In this toy example, "obs" takes the place of the observed parameter, and
"pred" takes the place of its approximation by a model. We are interested
in determining the quality of this approximation for predicting the value '0'.

user=# SELECT mf_binary_classifier('test_set','pred','obs','0','out_table');
 mf_binary_classifier   
----------------------
(1 row)

user=# \x
Expanded display is on.
user=# SELECT * FROM out_table;
-[ RECORD 1 ]---------------
tp  | 1894
tn  | 3878
fp  | 2726
fn  | 5902
tpr | 0.24294510005130836326
tnr | 0.58721986674742580254
ppv | 0.40995670995670995671
npv | 0.39652351738241308793
fpr | 0.41278013325257419746
fdr | 0.59004329004329004329
fnr | 0.75705489994869163674
acc | 0.40083333333333333333
f1  | 0.30509020618556701031
auc | 0.41508248339936708290

user=# \x
Expanded display is off.

We now repeat the same analysis, but using "m1" and "m2" as grouping columns.

user=# SELECT mf_binary_classifier('test_set','pred','obs','0','out_table2',
user-#                             'm1,m2');
 mf_binary_classifier   
----------------------
(1 row)

user=# \x
Expanded display is on.
user=# SELECT * FROM out_table2;
-[ RECORD 1 ]---------------
tp  | 400
tn  | 800
fp  | 800
fn  | 1600
m1  | 1
m2  | 1
tpr | 0.20000000000000000000
tnr | 0.50000000000000000000
ppv | 0.33333333333333333333
npv | 0.33333333333333333333
fpr | 0.50000000000000000000
fdr | 0.66666666666666666667
fnr | 0.80000000000000000000
acc | 0.33333333333333333333
f1  | 0.25000000000000000000
auc | 0.35000000000000000000
-[ RECORD 2 ]---------------
tp  | 900
tn  | 0
fp  | 900
fn  | 1800
m1  | 0
m2  | 1
tpr | 0.33333333333333333333
tnr | 0.00000000000000000000
ppv | 0.50000000000000000000
npv | 0.00000000000000000000
fpr | 1.00000000000000000000
fdr | 0.50000000000000000000
fnr | 0.66666666666666666667
acc | 0.25000000000000000000
f1  | 0.40000000000000000000
auc | 0.16666666666666666667
-[ RECORD 3 ]---------------
tp  | 144
tn  | 1728
fp  | 576
fn  | 1152
m1  | 1
m2  | 2
tpr | 0.11111111111111111111
tnr | 0.75000000000000000000
ppv | 0.20000000000000000000
npv | 0.60000000000000000000
fpr | 0.25000000000000000000
fnr | 0.88888888888888888889
acc | 0.52000000000000000000
f1  | 0.14285714285714285714
auc | 0.43055555555555555556
-[ RECORD 4 ]---------------
tp  | 450
tn  | 1350
fp  | 450
fn  | 1350
m1  | 0
m2  | 2
tpr | 0.25000000000000000000
tnr | 0.75000000000000000000
ppv | 0.50000000000000000000
npv | 0.50000000000000000000
fpr | 0.25000000000000000000
fdr | 0.50000000000000000000
fnr | 0.75000000000000000000
acc | 0.50000000000000000000
f1  | 0.33333333333333333333
auc | 0.50000000000000000000

user=# \x
Expanded display is off.
$ABC$::TEXT
END;
$$ LANGUAGE SQL;

